// Copyright (C) 2018-2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <vector>
#include <memory>
#include <string>
#include <samples/common.hpp>

#include <inference_engine.hpp>
#include <details/os/os_filesystem.hpp>
#include <samples/ocv_common.hpp>
#include <samples/classification_results.h>

using namespace InferenceEngine;

#if defined(ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
#define tcout std::wcout
#define file_name_t std::wstring
#define WEIGHTS_EXT L".bin"
#define imread_t imreadW
#define ClassificationResult_t ClassificationResultW
#else
#define tcout std::cout
#define file_name_t std::string
#define WEIGHTS_EXT ".bin"
#define imread_t cv::imread
#define ClassificationResult_t ClassificationResult
#endif

#if defined(ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
cv::Mat imreadW(std::wstring input_image_path) {
    cv::Mat image;
    std::ifstream input_image_stream;
    input_image_stream.open(
        input_image_path.c_str(),
        std::iostream::binary | std::ios_base::ate | std::ios_base::in);
    if (input_image_stream.is_open()) {
        if (input_image_stream.good()) {
            std::size_t file_size = input_image_stream.tellg();
            input_image_stream.seekg(0, std::ios::beg);
            std::vector<char> buffer(0);
            std::copy(
                std::istream_iterator<char>(input_image_stream),
                std::istream_iterator<char>(),
                std::back_inserter(buffer));
            image = cv::imdecode(cv::Mat(1, file_size, CV_8UC1, &buffer[0]), cv::IMREAD_COLOR);
        } else {
            tcout << "Input file '" << input_image_path << "' processing error" << std::endl;
        }
        input_image_stream.close();
    } else {
        tcout << "Unable to read input file '" << input_image_path << "'" << std::endl;
    }
    return image;
}

int wmain(int argc, wchar_t *argv[]) {
#else
std::vector<std::string> readTxt(std::string file)
{
    std::vector<std::string> rtv;
    std::ifstream infile; 
    infile.open(file.data());
    assert(infile.is_open());

    std::string s;
    while(getline(infile, s)) {
        rtv.push_back(s);
        std::cout << s << std::endl;
    }
    infile.close();
    return rtv;
}

int main(int argc, char *argv[]) {
#endif
    try {
        // ------------------------------ Parsing and validation of input args ---------------------------------
        if (argc != 4) {
            tcout << "Usage : ./hello_classification <path_to_model> <path_to_image list> <device_name>" << std::endl;
            return EXIT_FAILURE;
        }

        const file_name_t input_model{argv[1]};
        const file_name_t input_image_list{argv[2]};
        file_name_t input_image_path;
#if defined(ENABLE_UNICODE_PATH_SUPPORT) && defined(_WIN32)
        const std::string device_name = InferenceEngine::details::wStringtoMBCSstringChar(argv[3]);
#else
        const std::string device_name{argv[3]};
#endif
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 1. Load inference engine instance -------------------------------------
        Core ie;
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
        // mtcnn Rnet
        CNNNetwork network = ie.ReadNetwork(input_model, input_model.substr(0, input_model.size() - 4) + WEIGHTS_EXT);
        network.setBatchSize(1);
        float RnetThd = 0.0;
        // mtcnn Onet
        std::string oNet = input_model.substr(0, input_model.size() - 5) + "3.xml";
        CNNNetwork networkOnet = ie.ReadNetwork(oNet, oNet.substr(0, oNet.size() - 4) + WEIGHTS_EXT);
        networkOnet.setBatchSize(1);
        float OnetThd = 0.0;
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 3. Configure input & output ---------------------------------------------
        // --------------------------- Prepare input blobs -----------------------------------------------------
        // Rnet
        InputInfo::Ptr input_info = network.getInputsInfo().begin()->second;
        std::string input_name = network.getInputsInfo().begin()->first;
        /* Mark input as resizable by setting of a resize algorithm.
         * In this case we will be able to set an input blob of any shape to an infer request.
         * Resize and layout conversions are executed automatically during inference */
        input_info->getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR);
        input_info->setLayout(Layout::NCHW);
        input_info->setPrecision(Precision::U8);
        // Onet
        input_info = networkOnet.getInputsInfo().begin()->second;
        std::string input_name_Onet = networkOnet.getInputsInfo().begin()->first;
        input_info->getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR);
        input_info->setLayout(Layout::NCHW);
        input_info->setPrecision(Precision::U8);
        // --------------------------- Prepare output blobs ----------------------------------------------------
        // Rnet
        int outSize = network.getOutputsInfo().size();
        DataPtr output_info[outSize];
        std::string output_name[outSize];
        auto iter = network.getOutputsInfo().begin();
        for (int i = 0; i < outSize; iter++, i++) {
            output_info[i] = iter->second;
            output_name[i] = iter->first;
            output_info[i]->setPrecision(Precision::FP32);
        }
        // Onet
        outSize = networkOnet.getOutputsInfo().size();
        DataPtr output_info_Onet[outSize];
        std::string output_name_Onet[outSize];
        iter = networkOnet.getOutputsInfo().begin();
        for (int i = 0; i < outSize; iter++, i++) {
            output_info_Onet[i] = iter->second;
            output_name_Onet[i] = iter->first;
            output_info_Onet[i]->setPrecision(Precision::FP32);
        }
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 4. Loading model to the device ------------------------------------------
        ExecutableNetwork executable_network = ie.LoadNetwork(network, device_name);
        ExecutableNetwork executable_network_Onet = ie.LoadNetwork(networkOnet, device_name);
        // -----------------------------------------------------------------------------------------------------

        // --------------------------- 5. Create infer request -------------------------------------------------
        // Rnet
        InferRequest infer_request = executable_network.CreateInferRequest();
        SizeVector blobSize = infer_request.GetBlob(input_name)->getTensorDesc().getDims();
        // Onet
        InferRequest infer_request_Onet = executable_network_Onet.CreateInferRequest();
        blobSize = infer_request_Onet.GetBlob(input_name_Onet)->getTensorDesc().getDims();
        // -----------------------------------------------------------------------------------------------------
        // test
        std::vector<std::string> lists = readTxt(input_image_list);
        // int cnt = 0;
        // cv::Point2f p1(0, 0);
        // cv::Point2f p2(0, 0);
        // cv::Point2f p3(0, 0);
        // cv::Point2f p4(0, 0);
        int imgCol = 0;
        int imgRow = 0;
        cv::Rect roiBuf[2];
        // default (contain face)
        roiBuf[0].x = 400;
        roiBuf[0].y = 150;
        roiBuf[0].width = 200;
        roiBuf[0].height = 200;
        // default (contain non-face)
        roiBuf[1].x = 100;
        roiBuf[1].y = 150;
        roiBuf[1].width = 200;
        roiBuf[1].height = 200;
        

        using myBlobType = InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type;
        for(auto it : lists) {
            input_image_path = it;
            // --------------------------- 6. Prepare input --------------------------------------------------------
            /* Read input image to a blob and set it to an infer request without resize and layout conversions. */
            // for show
            cv::Mat oriImage = imread_t(input_image_path);

            // for crop
            cv::Mat image = imread_t(input_image_path);
            imgCol = image.cols;
            imgRow = image.rows;
            tcout << imgCol << std::endl;
            tcout << imgRow << std::endl;
            for (auto roi : roiBuf) {
                cv::rectangle(oriImage, roi, cv::Scalar(0, 0, 255), 3, 8, 0);
                // Rnet
                cv::Mat RnetImage;
                cv::Mat crop = image(roi);

                cv::cvtColor(crop, RnetImage, cv::COLOR_BGR2RGB);
                RnetImage = RnetImage.t();
                Blob::Ptr imgBlob = wrapMat2Blob(RnetImage);  // just wrap Mat data by Blob::Ptr without allocating of new memory
                infer_request.SetBlob(input_name, imgBlob);  // infer_request accepts input blob of any size
                // -----------------------------------------------------------------------------------------------------

                // --------------------------- 7. Do inference --------------------------------------------------------
                /* Running the request synchronously */
                infer_request.Infer();
                // -----------------------------------------------------------------------------------------------------
                // --------------------------- 8. Process output ------------------------------------------------------
                // face rect offset
                Blob::Ptr output = infer_request.GetBlob(output_name[0]);
                // face score: index 1 for true
                Blob::Ptr output2 = infer_request.GetBlob(output_name[1]);

                InferenceEngine::Blob::Ptr outBlob = std::move(output2);
                InferenceEngine::TBlob<myBlobType>& tblob = dynamic_cast<InferenceEngine::TBlob<myBlobType>&>(*outBlob);

                InferenceEngine::Blob::Ptr outBlob_offset = std::move(output);
                InferenceEngine::TBlob<myBlobType>& tblob_offset = dynamic_cast<InferenceEngine::TBlob<myBlobType>&>(*outBlob_offset);
                InferenceEngine::SizeVector dims = tblob_offset.getTensorDesc().getDims();

                myBlobType* batchData = tblob.data();
                myBlobType* Rnet_offset = tblob_offset.data();

                if (RnetThd < batchData[1]) {
                    cv::Mat OnetImage;
                    roi.x = int(roi.x + roi.width * Rnet_offset[0]);
                    roi.y = int(roi.y + roi.height * Rnet_offset[1]);
                    roi.width += int(roi.width * (-Rnet_offset[0] + Rnet_offset[2]));
                    roi.height += int(roi.height * (-Rnet_offset[1] + Rnet_offset[3]));
                    crop = image(roi);
                    cv::rectangle(oriImage, roi, cv::Scalar(255, 0, 0), 3, 8, 0);
                    // cv::imshow("crop2", oriImage);
                    // cv::waitKey();

                    cv::cvtColor(crop, OnetImage, cv::COLOR_BGR2RGB);
                    OnetImage = OnetImage.t();
                    Blob::Ptr OnetImgBlob = wrapMat2Blob(OnetImage);
                    infer_request_Onet.SetBlob(input_name_Onet, OnetImgBlob);
                    infer_request_Onet.Infer();
                    // face rect offset
                    Blob::Ptr output_Onet = infer_request_Onet.GetBlob(output_name_Onet[0]);
                    // 5 landmarks of face, 0:5 -> x, 5:10 -> y
                    // Upright face with index:
                    // 3     0
                    //    2
                    // 4     1
                    Blob::Ptr output2_Onet = infer_request_Onet.GetBlob(output_name_Onet[1]);
                    // face score: index 1 for true
                    Blob::Ptr output3_Onet = infer_request_Onet.GetBlob(output_name_Onet[2]);

                    InferenceEngine::Blob::Ptr outBlob_Onet = std::move(output3_Onet);
                    InferenceEngine::TBlob<myBlobType>& tblob_Onet = dynamic_cast<InferenceEngine::TBlob<myBlobType>&>(*outBlob_Onet);
                    InferenceEngine::SizeVector dims = tblob_Onet.getTensorDesc().getDims();

                    myBlobType* OnetData = tblob_Onet.data();
                    if (OnetThd < OnetData[1]) {
                        // landmark
                        InferenceEngine::Blob::Ptr outBlob_landmark = std::move(output2_Onet);
                        InferenceEngine::TBlob<myBlobType>& tblob_landmark = dynamic_cast<InferenceEngine::TBlob<myBlobType>&>(*outBlob_landmark);
                        dims = tblob_landmark.getTensorDesc().getDims();
                        myBlobType* landmarkData = tblob_landmark.data();

                        for (int i = 0; i < int(dims[1] / 2); i++) {
                            cv::circle(oriImage, cv::Point(roi.x + int(landmarkData[i] * roi.width), roi.y + int(landmarkData[i + 5] * roi.height)), 3, cv::Scalar(0, 255, 0), -1);
                        }

                        // Onet offset
                        InferenceEngine::Blob::Ptr out_Onet_offset = std::move(output_Onet);
                        InferenceEngine::TBlob<myBlobType>& tblob_Onet_offset = dynamic_cast<InferenceEngine::TBlob<myBlobType>&>(*out_Onet_offset);
                        dims = tblob_Onet_offset.getTensorDesc().getDims();
                        myBlobType* Onet_offset = tblob_Onet_offset.data();

                        roi.x = int(roi.x + roi.width * Onet_offset[0]);
                        roi.y = int(roi.y + roi.height * Onet_offset[1]);
                        roi.width += int(roi.width * (-Onet_offset[0] + Onet_offset[2]));
                        roi.height += int(roi.height * (-Onet_offset[1] + Onet_offset[3]));
                        cv::rectangle(oriImage, roi, cv::Scalar(0, 255, 0), 3, 8, 0);
                        cv::imshow("crop3", oriImage);
                        cv::waitKey();
                        // calc
                        // cv::Point2f center = cv::Point2f(landmarkData[2], landmarkData[7]);
                        // cv::Point2f rEye = cv::Point2f(landmarkData[0], landmarkData[5]);
                        // cv::Point2f rMouth = cv::Point2f(landmarkData[1], landmarkData[6]);
                        // cv::Point2f lEye = cv::Point2f(landmarkData[3], landmarkData[8]);
                        // cv::Point2f lMouth = cv::Point2f(landmarkData[4], landmarkData[9]);
                        // p1 += (rEye - center);
                        // p2 += (lEye - center);
                        // p3 += (lMouth - center);
                        // p4 += (rMouth - center);
                        // cnt++;
                    }
                } else {
                    ;
                }
            }
        }
        // p1 = p1 / cnt;
        // p2 = p2 / cnt;
        // p3 = p3 / cnt;
        // p4 = p4 / cnt;
        // tcout << p1 << std::endl;
        // tcout << p2 << std::endl;
        // tcout << p3 << std::endl;
        // tcout << p4 << std::endl;
        // tcout << cnt << std::endl;
        // neg
        // [-0.153565, -0.267271]
        // [-0.123495, 0.208766]
        // [0.135818, 0.194164]
        // [0.18123, -0.299656]
        // -----------------------------------------------------------------------------------------------------
    } catch (const std::exception & ex) {
        std::cerr << ex.what() << std::endl;
        return EXIT_FAILURE;
    }
    std::cout << "This sample is an API example, for any performance measurements "
                 "please use the dedicated benchmark_app tool" << std::endl;
    return EXIT_SUCCESS;
}
